tags:
  autoscaling: true
  loadBalancing: true

image:
  imageName: nvcr.io/nvidia/tritonserver:23.08-py3
  pullPolicy: Always
  resources:
    limits:
      # nvidia.com/mig-4g.24gb: 1
      nvidia.com/gpu: 1

devimage:
  imageName: nvcr.io/nvidia/tritonserver:23.08-py3-sdk
  pullPolicy: Always
  resources:
    requests:
      memory: "2Gi"
      cpu: "2500m"
    limits:
      memory: "16Gi"
      cpu: "5000m"

traefik:
  ports:
    triton-http:
      port: 18000
      exposedPort: 8000
      expose: true
      protocol: TCP
    triton-grpc:
      port: 18001
      exposedPort: 8001
      expose: true
      protocol: TCP
  global:
    sendAnonymousUsage: false

autoscaling:
  minReplicas: 1
  maxReplicas: 2
  metrics:
    - type: Pods
      pods:
        metric:
          name: avg_time_queue_us
        target:
          type: AverageValue
          averageValue: !!int 200000

prometheus-adapter:
  prometheus:
    url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local
    port: 9090
  rules:
    custom:
      - seriesQuery: 'nv_inference_queue_duration_us{namespace="default",pod!=""}'
        resources:
          overrides:
            namespace:
              resource: "namespace"
            pod:
              resource: "pod"
        name:
          matches: "nv_inference_queue_duration_us"
          as: "avg_time_queue_us"
        metricsQuery: "avg(rate(nv_inference_queue_duration_us{<<.LabelMatchers>>}[30s])/(1+delta(nv_inference_request_success{<<.LabelMatchers>>}[30s]))) by (<<.GroupBy>>)"

imageCredentials:
  registry: nvcr.io
  username: $oauthtoken
  password: 'api-key'
  email: 'user@nvidia.com'
